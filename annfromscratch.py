# -*- coding: utf-8 -*-
"""ANNfromScratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11lImkx1qKqCTrdEMS41T8KwGvu0eFL0v
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/drive/My Drive/dataset/diabetes-DBP.csv')
df.head()

#Features Selection
X = df #include all columns
Y = df[['Outcome']] #target variable; only Outcome column
X = X.drop(['Outcome'], axis = 1)
print(X.shape[1])

#Split the dataset into training-test set
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)

"""Initialize Hidden Layers"""

#Initialization of Parameters
def parameter_initialization(nx,nh,ny):

  #Random initialization of weights matrix of shape layers_dimension[l] and biases vector of shape layers-dimension[l-1]
  W1 = np.random.randn(nh, nx)*0.01
  b1 = np.zeros((nh,1))
  W2 = np.random.randn(nh, nh)*0.01
  b2 = np.zeros((nh,1))
  W3 = np.random.randn(ny, nh)*0.01
  b3 = np.zeros((ny,1))

  return {"W1": W1, "b1": b1, "W2": W2, "b2": b2, "W3": W3, "b3": b3}

"""Forward Propagation"""

#Activation function - sigmoid
def sigmoid(z):
    s = 1/(1+np.exp(-z))
    return s

def forward_propagation(X, parameters):

    #Formul z = wx + b; A = sigmoid(z)
    Z1 = np.dot(parameters['W1'], X) + parameters['b1']
    A1 = sigmoid(Z1)

    Z2 = np.dot(parameters['W2'], A1) + parameters['b2']
    A2 = sigmoid(Z2)

    Z3 = np.dot(parameters['W3'], A2) + parameters['b3']
    A3 = sigmoid(Z3)

    return {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2, "Z3": Z3, "A3": A3}

"""Cost Calculation"""

def cost_estimation(true, predicted):

  log_probs = np.multiply(np.log(predicted), true) + np.multiply(np.log(1 - predicted), (1 - true))
  total_cost = -np.sum(log_probs)/true.shape[1]

  return np.squeeze(total_cost)

"""Backward Propagation"""

#Backward Propagation function
def backward_propagation(parameters, activations, X, Y):
  '''
  dwl = dzl*(A[l-1].T) /m
  dbl = sum(dzli)/m
  dzl = dAl*g'(zl)
  '''
  m = X.shape[1]

    #Output layer
  dZ3 = activations['A3'] - Y
  dW3 = np.dot(dZ3, activations['A2'].T) / m
  db3 = np.sum(dZ3, axis=1, keepdims = True)/m

    #Hidden layer 2
  dZ2 = np.dot(parameters['W3'].T, dZ3)*(activations['A2']-np.power(activations['A2'], 2))
  dW2 = np.dot(dZ2, activations['A1'].T)/m
  db2 = np.sum(dZ2, axis=1,keepdims=True)/m

    #Hidden layer 1
  dZ1 = np.dot(parameters['W2'].T, dZ2)*(activations['A1']-np.power(activations['A1'], 2))
  dW1 = np.dot(dZ1, X.T)/m
  db1 = np.sum(dZ1, axis=1,keepdims=True)/m

  return {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2, "dW3": dW3, "db3": db3}

 def parameters_update(parameters, gradients, alpha = 0.01):
    #parameters update rules w = w - learning_rate * dw ; b = b - learning_rate * db
    parameters['W1'] = parameters['W1'] - alpha * gradients['dW1']
    parameters['b1'] = parameters['b1'] - alpha * gradients['db1']
    parameters['W2'] = parameters['W2'] - alpha * gradients['dW2']
    parameters['b2'] = parameters['b2'] - alpha * gradients['db2']
    parameters['W3'] = parameters['W3'] - alpha * gradients['dW3']
    parameters['b3'] = parameters['b3'] - alpha * gradients['db3']

    return parameters

def neural_network_model(X,Y, n_h = 10, epochs = 100):
  n_x = X.shape[0]
  n_y = Y.shape[0]

  parameters = parameter_initialization(n_x, n_h, n_y)
  for i in range(0,epochs):
    results = forward_propagation(X, parameters)
    errors = cost_estimation(Y, results['A3'])
    grads = backward_propagation(parameters, results, X,Y)
    params = parameters_update(parameters, grads)

  return parameters

x = X_train.T   #(614,8) -> (8,614)
y = y_train.values.reshape(1, y_train.size) #(614,1) -> (1,614)
model = neural_network_model(x,y, n_h = 10, epochs = 100)

def predict(parameters, X):
    results = forward_propagation(X, parameters)
    #print (results['A3'][0])
    predictions = np.around(results['A3'])
    return predictions

predictions = predict(model, x)
print ('Accuracy: %d' % float((np.dot(y,predictions.T) + np.dot(1-y,1-predictions.T))/float(y.size)*100) + '%')